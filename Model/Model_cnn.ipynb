{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5213b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc6a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data      = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rescale=1./255,shear_range=0.02,zoom_range=0.02,)\n",
    "test_data       = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b37d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 images belonging to 2 classes.\n",
      "Found 126 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_x_y = train_data.flow_from_directory(directory= r'C:\\Users\\ANANTHU\\Desktop\\solar\\dataset1\\training_set',\n",
    "                                           target_size=(64, 64),\n",
    "                                           color_mode='rgb',\n",
    "                                           class_mode='categorical',\n",
    "                                           batch_size=32,\n",
    "                                           interpolation=\"bicubic\",)\n",
    "test_x_y = test_data.flow_from_directory(directory= r'C:\\Users\\ANANTHU\\Desktop\\solar\\dataset1\\test_set',\n",
    "                                           target_size=(64, 64),\n",
    "                                           color_mode='rgb',\n",
    "                                           class_mode='categorical',\n",
    "                                           batch_size=32,\n",
    "                                           interpolation=\"bicubic\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20af515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Defective': 0, 'Non-Defective': 1}, {'Defective': 0, 'Non-Defective': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x_y.class_indices),(test_x_y.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70b2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86378813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "base_model = Sequential()\n",
    "# Step 1 - Convolution\n",
    "base_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64,64,3))) \n",
    "# Step 2 - Pooling\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "base_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Adding a third convolutional layer\n",
    "base_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Step 3 - Flattening\n",
    "base_model.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "base_model.add(Dense(256, activation='relu'))\n",
    "base_model.add(Dense(128, activation='relu'))\n",
    "base_model.add(Dense(64, activation='relu'))\n",
    "base_model.add(Dense(32, activation='relu'))\n",
    "\n",
    "base_model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc8f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978db6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2097408   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,233,954\n",
      "Trainable params: 2,233,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a39b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b312fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('best_model_improved.h5',\n",
    "                            monitor = 'val_loss',\n",
    "                            verbose = 1,\n",
    "                            save_best_only= True,\n",
    "                            mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "282d5867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6993 - accuracy: 0.5444\n",
      "Epoch 00001: val_loss improved from inf to 0.71117, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 22s 2s/step - loss: 0.6993 - accuracy: 0.5444 - val_loss: 0.7112 - val_accuracy: 0.4048\n",
      "Epoch 2/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6899 - accuracy: 0.5503\n",
      "Epoch 00002: val_loss did not improve from 0.71117\n",
      "6/6 [==============================] - 3s 531ms/step - loss: 0.6899 - accuracy: 0.5503 - val_loss: 0.7518 - val_accuracy: 0.4048\n",
      "Epoch 3/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5444\n",
      "Epoch 00003: val_loss did not improve from 0.71117\n",
      "6/6 [==============================] - 3s 571ms/step - loss: 0.6926 - accuracy: 0.5444 - val_loss: 0.7173 - val_accuracy: 0.4048\n",
      "Epoch 4/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.5858\n",
      "Epoch 00004: val_loss improved from 0.71117 to 0.69950, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 6s 974ms/step - loss: 0.6898 - accuracy: 0.5858 - val_loss: 0.6995 - val_accuracy: 0.4127\n",
      "Epoch 5/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6845 - accuracy: 0.5858\n",
      "Epoch 00005: val_loss did not improve from 0.69950\n",
      "6/6 [==============================] - 3s 501ms/step - loss: 0.6845 - accuracy: 0.5858 - val_loss: 0.7215 - val_accuracy: 0.4048\n",
      "Epoch 6/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.5976\n",
      "Epoch 00006: val_loss did not improve from 0.69950\n",
      "6/6 [==============================] - 3s 558ms/step - loss: 0.6767 - accuracy: 0.5976 - val_loss: 0.7629 - val_accuracy: 0.3968\n",
      "Epoch 7/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6835 - accuracy: 0.5858\n",
      "Epoch 00007: val_loss did not improve from 0.69950\n",
      "6/6 [==============================] - 4s 592ms/step - loss: 0.6835 - accuracy: 0.5858 - val_loss: 0.7684 - val_accuracy: 0.4048\n",
      "Epoch 8/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6578 - accuracy: 0.6272\n",
      "Epoch 00008: val_loss improved from 0.69950 to 0.69873, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 5s 842ms/step - loss: 0.6578 - accuracy: 0.6272 - val_loss: 0.6987 - val_accuracy: 0.4206\n",
      "Epoch 9/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6635 - accuracy: 0.6036\n",
      "Epoch 00009: val_loss did not improve from 0.69873\n",
      "6/6 [==============================] - 4s 595ms/step - loss: 0.6635 - accuracy: 0.6036 - val_loss: 0.7405 - val_accuracy: 0.4048\n",
      "Epoch 10/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.6154\n",
      "Epoch 00010: val_loss did not improve from 0.69873\n",
      "6/6 [==============================] - 3s 558ms/step - loss: 0.6540 - accuracy: 0.6154 - val_loss: 0.7082 - val_accuracy: 0.4127\n",
      "Epoch 11/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.5917\n",
      "Epoch 00011: val_loss did not improve from 0.69873\n",
      "6/6 [==============================] - 3s 578ms/step - loss: 0.6472 - accuracy: 0.5917 - val_loss: 0.7011 - val_accuracy: 0.4048\n",
      "Epoch 12/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6458 - accuracy: 0.6272\n",
      "Epoch 00012: val_loss did not improve from 0.69873\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 0.6458 - accuracy: 0.6272 - val_loss: 0.7023 - val_accuracy: 0.4286\n",
      "Epoch 13/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6393 - accuracy: 0.5740\n",
      "Epoch 00013: val_loss improved from 0.69873 to 0.67826, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 683ms/step - loss: 0.6393 - accuracy: 0.5740 - val_loss: 0.6783 - val_accuracy: 0.4206\n",
      "Epoch 14/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.6154\n",
      "Epoch 00014: val_loss did not improve from 0.67826\n",
      "6/6 [==============================] - 4s 651ms/step - loss: 0.6357 - accuracy: 0.6154 - val_loss: 0.7372 - val_accuracy: 0.3889\n",
      "Epoch 15/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.6154\n",
      "Epoch 00015: val_loss did not improve from 0.67826\n",
      "6/6 [==============================] - 4s 571ms/step - loss: 0.6157 - accuracy: 0.6154 - val_loss: 0.7046 - val_accuracy: 0.4127\n",
      "Epoch 16/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5966 - accuracy: 0.6272\n",
      "Epoch 00016: val_loss did not improve from 0.67826\n",
      "6/6 [==============================] - 4s 579ms/step - loss: 0.5966 - accuracy: 0.6272 - val_loss: 0.7625 - val_accuracy: 0.4048\n",
      "Epoch 17/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6756 - accuracy: 0.5621\n",
      "Epoch 00017: val_loss improved from 0.67826 to 0.66028, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 679ms/step - loss: 0.6756 - accuracy: 0.5621 - val_loss: 0.6603 - val_accuracy: 0.6905\n",
      "Epoch 18/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6665 - accuracy: 0.5385\n",
      "Epoch 00018: val_loss improved from 0.66028 to 0.65666, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 635ms/step - loss: 0.6665 - accuracy: 0.5385 - val_loss: 0.6567 - val_accuracy: 0.6508\n",
      "Epoch 19/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.6343 - accuracy: 0.6154\n",
      "Epoch 00019: val_loss did not improve from 0.65666\n",
      "6/6 [==============================] - 3s 565ms/step - loss: 0.6343 - accuracy: 0.6154 - val_loss: 0.6947 - val_accuracy: 0.4206\n",
      "Epoch 20/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.6509\n",
      "Epoch 00020: val_loss did not improve from 0.65666\n",
      "6/6 [==============================] - 3s 555ms/step - loss: 0.5945 - accuracy: 0.6509 - val_loss: 0.6643 - val_accuracy: 0.5714\n",
      "Epoch 21/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.6568\n",
      "Epoch 00021: val_loss did not improve from 0.65666\n",
      "6/6 [==============================] - 3s 549ms/step - loss: 0.5854 - accuracy: 0.6568 - val_loss: 0.7623 - val_accuracy: 0.4127\n",
      "Epoch 22/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5398 - accuracy: 0.6923\n",
      "Epoch 00022: val_loss improved from 0.65666 to 0.61866, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 852ms/step - loss: 0.5398 - accuracy: 0.6923 - val_loss: 0.6187 - val_accuracy: 0.6905\n",
      "Epoch 23/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.7278\n",
      "Epoch 00023: val_loss did not improve from 0.61866\n",
      "6/6 [==============================] - 3s 516ms/step - loss: 0.5169 - accuracy: 0.7278 - val_loss: 0.6301 - val_accuracy: 0.6587\n",
      "Epoch 24/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.7160\n",
      "Epoch 00024: val_loss did not improve from 0.61866\n",
      "6/6 [==============================] - 4s 621ms/step - loss: 0.5450 - accuracy: 0.7160 - val_loss: 0.6994 - val_accuracy: 0.5079\n",
      "Epoch 25/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7278\n",
      "Epoch 00025: val_loss improved from 0.61866 to 0.56627, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 5s 927ms/step - loss: 0.5025 - accuracy: 0.7278 - val_loss: 0.5663 - val_accuracy: 0.6429\n",
      "Epoch 26/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.7574\n",
      "Epoch 00026: val_loss improved from 0.56627 to 0.52055, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 622ms/step - loss: 0.4814 - accuracy: 0.7574 - val_loss: 0.5206 - val_accuracy: 0.6984\n",
      "Epoch 27/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.7396\n",
      "Epoch 00027: val_loss did not improve from 0.52055\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 0.4679 - accuracy: 0.7396 - val_loss: 0.6424 - val_accuracy: 0.6349\n",
      "Epoch 28/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.7692\n",
      "Epoch 00028: val_loss improved from 0.52055 to 0.48673, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 660ms/step - loss: 0.4206 - accuracy: 0.7692 - val_loss: 0.4867 - val_accuracy: 0.7778\n",
      "Epoch 29/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.7929\n",
      "Epoch 00029: val_loss improved from 0.48673 to 0.39028, saving model to best_model_improved.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 4s 777ms/step - loss: 0.3586 - accuracy: 0.7929 - val_loss: 0.3903 - val_accuracy: 0.7937\n",
      "Epoch 30/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3277 - accuracy: 0.8107\n",
      "Epoch 00030: val_loss improved from 0.39028 to 0.33713, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 754ms/step - loss: 0.3277 - accuracy: 0.8107 - val_loss: 0.3371 - val_accuracy: 0.8333\n",
      "Epoch 31/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3738 - accuracy: 0.7811\n",
      "Epoch 00031: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 642ms/step - loss: 0.3738 - accuracy: 0.7811 - val_loss: 0.3657 - val_accuracy: 0.8175\n",
      "Epoch 32/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.8284\n",
      "Epoch 00032: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 558ms/step - loss: 0.3349 - accuracy: 0.8284 - val_loss: 0.7678 - val_accuracy: 0.7143\n",
      "Epoch 33/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4953 - accuracy: 0.7692\n",
      "Epoch 00033: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 560ms/step - loss: 0.4953 - accuracy: 0.7692 - val_loss: 0.5640 - val_accuracy: 0.7063\n",
      "Epoch 34/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4735 - accuracy: 0.6982\n",
      "Epoch 00034: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 559ms/step - loss: 0.4735 - accuracy: 0.6982 - val_loss: 0.6968 - val_accuracy: 0.6032\n",
      "Epoch 35/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.4273 - accuracy: 0.8047\n",
      "Epoch 00035: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 531ms/step - loss: 0.4273 - accuracy: 0.8047 - val_loss: 0.4952 - val_accuracy: 0.7143\n",
      "Epoch 36/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.7929\n",
      "Epoch 00036: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 522ms/step - loss: 0.3910 - accuracy: 0.7929 - val_loss: 0.5188 - val_accuracy: 0.7857\n",
      "Epoch 37/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.7870\n",
      "Epoch 00037: val_loss did not improve from 0.33713\n",
      "6/6 [==============================] - 3s 544ms/step - loss: 0.3601 - accuracy: 0.7870 - val_loss: 0.4108 - val_accuracy: 0.8095\n",
      "Epoch 38/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8107 ETA: 0s - loss: 0.3314 - accuracy\n",
      "Epoch 00038: val_loss improved from 0.33713 to 0.33049, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 726ms/step - loss: 0.3065 - accuracy: 0.8107 - val_loss: 0.3305 - val_accuracy: 0.7857\n",
      "Epoch 39/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.8698\n",
      "Epoch 00039: val_loss improved from 0.33049 to 0.29213, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 680ms/step - loss: 0.2747 - accuracy: 0.8698 - val_loss: 0.2921 - val_accuracy: 0.8333\n",
      "Epoch 40/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.8935\n",
      "Epoch 00040: val_loss did not improve from 0.29213\n",
      "6/6 [==============================] - 3s 517ms/step - loss: 0.2560 - accuracy: 0.8935 - val_loss: 0.4136 - val_accuracy: 0.8413\n",
      "Epoch 41/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.8639\n",
      "Epoch 00041: val_loss improved from 0.29213 to 0.26774, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 747ms/step - loss: 0.2531 - accuracy: 0.8639 - val_loss: 0.2677 - val_accuracy: 0.8333\n",
      "Epoch 42/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.8876\n",
      "Epoch 00042: val_loss improved from 0.26774 to 0.24258, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 815ms/step - loss: 0.2219 - accuracy: 0.8876 - val_loss: 0.2426 - val_accuracy: 0.8571\n",
      "Epoch 43/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9112\n",
      "Epoch 00043: val_loss did not improve from 0.24258\n",
      "6/6 [==============================] - 3s 577ms/step - loss: 0.1934 - accuracy: 0.9112 - val_loss: 0.2980 - val_accuracy: 0.8651\n",
      "Epoch 44/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9112\n",
      "Epoch 00044: val_loss did not improve from 0.24258\n",
      "6/6 [==============================] - 3s 540ms/step - loss: 0.1915 - accuracy: 0.9112 - val_loss: 0.2866 - val_accuracy: 0.8571\n",
      "Epoch 45/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.8994\n",
      "Epoch 00045: val_loss did not improve from 0.24258\n",
      "6/6 [==============================] - 3s 509ms/step - loss: 0.2099 - accuracy: 0.8994 - val_loss: 0.2571 - val_accuracy: 0.8492\n",
      "Epoch 46/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.8994\n",
      "Epoch 00046: val_loss improved from 0.24258 to 0.21742, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 721ms/step - loss: 0.2017 - accuracy: 0.8994 - val_loss: 0.2174 - val_accuracy: 0.8968\n",
      "Epoch 47/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9231\n",
      "Epoch 00047: val_loss did not improve from 0.21742\n",
      "6/6 [==============================] - 4s 586ms/step - loss: 0.1797 - accuracy: 0.9231 - val_loss: 0.3613 - val_accuracy: 0.8889\n",
      "Epoch 48/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9231\n",
      "Epoch 00048: val_loss did not improve from 0.21742\n",
      "6/6 [==============================] - 4s 568ms/step - loss: 0.2041 - accuracy: 0.9231 - val_loss: 0.2241 - val_accuracy: 0.9127\n",
      "Epoch 49/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1865 - accuracy: 0.9053\n",
      "Epoch 00049: val_loss improved from 0.21742 to 0.18125, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 697ms/step - loss: 0.1865 - accuracy: 0.9053 - val_loss: 0.1813 - val_accuracy: 0.9048\n",
      "Epoch 50/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9527\n",
      "Epoch 00050: val_loss did not improve from 0.18125\n",
      "6/6 [==============================] - 3s 559ms/step - loss: 0.1483 - accuracy: 0.9527 - val_loss: 0.2829 - val_accuracy: 0.9127\n",
      "Epoch 51/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9527\n",
      "Epoch 00051: val_loss did not improve from 0.18125\n",
      "6/6 [==============================] - 3s 533ms/step - loss: 0.1146 - accuracy: 0.9527 - val_loss: 0.1961 - val_accuracy: 0.9683\n",
      "Epoch 52/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9467\n",
      "Epoch 00052: val_loss improved from 0.18125 to 0.13942, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 678ms/step - loss: 0.1059 - accuracy: 0.9467 - val_loss: 0.1394 - val_accuracy: 0.9524\n",
      "Epoch 53/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9645\n",
      "Epoch 00053: val_loss did not improve from 0.13942\n",
      "6/6 [==============================] - 3s 606ms/step - loss: 0.0832 - accuracy: 0.9645 - val_loss: 0.2884 - val_accuracy: 0.9206\n",
      "Epoch 54/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9586\n",
      "Epoch 00054: val_loss improved from 0.13942 to 0.13581, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 641ms/step - loss: 0.0722 - accuracy: 0.9586 - val_loss: 0.1358 - val_accuracy: 0.9603\n",
      "Epoch 55/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9645\n",
      "Epoch 00055: val_loss improved from 0.13581 to 0.12680, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 645ms/step - loss: 0.0943 - accuracy: 0.9645 - val_loss: 0.1268 - val_accuracy: 0.9841\n",
      "Epoch 56/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9704\n",
      "Epoch 00056: val_loss did not improve from 0.12680\n",
      "6/6 [==============================] - 3s 565ms/step - loss: 0.0693 - accuracy: 0.9704 - val_loss: 0.1466 - val_accuracy: 0.9603\n",
      "Epoch 57/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9645\n",
      "Epoch 00057: val_loss did not improve from 0.12680\n",
      "6/6 [==============================] - 3s 519ms/step - loss: 0.0582 - accuracy: 0.9645 - val_loss: 0.1278 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9704\n",
      "Epoch 00058: val_loss did not improve from 0.12680\n",
      "6/6 [==============================] - 3s 504ms/step - loss: 0.0448 - accuracy: 0.9704 - val_loss: 0.2641 - val_accuracy: 0.9286\n",
      "Epoch 59/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9704\n",
      "Epoch 00059: val_loss did not improve from 0.12680\n",
      "6/6 [==============================] - 3s 547ms/step - loss: 0.0618 - accuracy: 0.9704 - val_loss: 0.1474 - val_accuracy: 0.9444\n",
      "Epoch 60/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9941\n",
      "Epoch 00060: val_loss improved from 0.12680 to 0.10544, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 665ms/step - loss: 0.0294 - accuracy: 0.9941 - val_loss: 0.1054 - val_accuracy: 0.9762\n",
      "Epoch 61/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9941\n",
      "Epoch 00061: val_loss improved from 0.10544 to 0.05067, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 691ms/step - loss: 0.0336 - accuracy: 0.9941 - val_loss: 0.0507 - val_accuracy: 0.9841\n",
      "Epoch 62/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9763\n",
      "Epoch 00062: val_loss did not improve from 0.05067\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 0.0677 - accuracy: 0.9763 - val_loss: 0.0746 - val_accuracy: 0.9762\n",
      "Epoch 63/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9645\n",
      "Epoch 00063: val_loss improved from 0.05067 to 0.04288, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 737ms/step - loss: 0.1101 - accuracy: 0.9645 - val_loss: 0.0429 - val_accuracy: 0.9841\n",
      "Epoch 64/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9882\n",
      "Epoch 00064: val_loss did not improve from 0.04288\n",
      "6/6 [==============================] - 4s 593ms/step - loss: 0.0452 - accuracy: 0.9882 - val_loss: 0.1245 - val_accuracy: 0.9603\n",
      "Epoch 65/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9704\n",
      "Epoch 00065: val_loss did not improve from 0.04288\n",
      "6/6 [==============================] - 3s 506ms/step - loss: 0.0725 - accuracy: 0.9704 - val_loss: 0.0960 - val_accuracy: 0.9683\n",
      "Epoch 66/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9704\n",
      "Epoch 00066: val_loss improved from 0.04288 to 0.03997, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.0830 - accuracy: 0.9704 - val_loss: 0.0400 - val_accuracy: 0.9921\n",
      "Epoch 67/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 0.03997\n",
      "6/6 [==============================] - 3s 482ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9762\n",
      "Epoch 68/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9822\n",
      "Epoch 00068: val_loss did not improve from 0.03997\n",
      "6/6 [==============================] - 3s 515ms/step - loss: 0.0415 - accuracy: 0.9822 - val_loss: 0.0538 - val_accuracy: 0.9921\n",
      "Epoch 69/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9941\n",
      "Epoch 00069: val_loss did not improve from 0.03997\n",
      "6/6 [==============================] - 4s 586ms/step - loss: 0.0129 - accuracy: 0.9941 - val_loss: 0.0534 - val_accuracy: 0.9921\n",
      "Epoch 70/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 00070: val_loss did not improve from 0.03997\n",
      "6/6 [==============================] - 3s 506ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 0.9762\n",
      "Epoch 71/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9882\n",
      "Epoch 00071: val_loss improved from 0.03997 to 0.03615, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 626ms/step - loss: 0.0177 - accuracy: 0.9882 - val_loss: 0.0362 - val_accuracy: 0.9921\n",
      "Epoch 72/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 00072: val_loss improved from 0.03615 to 0.01510, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 721ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 0.9921\n",
      "Epoch 73/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9941\n",
      "Epoch 00073: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 4s 587ms/step - loss: 0.0072 - accuracy: 0.9941 - val_loss: 0.0770 - val_accuracy: 0.9841\n",
      "Epoch 74/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 00074: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 503ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0309 - val_accuracy: 0.9921\n",
      "Epoch 75/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9941\n",
      "Epoch 00075: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 494ms/step - loss: 0.0056 - accuracy: 0.9941 - val_loss: 0.0540 - val_accuracy: 0.9762\n",
      "Epoch 76/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
      "Epoch 00076: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 603ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0800 - val_accuracy: 0.9841\n",
      "Epoch 77/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.5953e-04 - accuracy: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 493ms/step - loss: 8.5953e-04 - accuracy: 1.0000 - val_loss: 0.0912 - val_accuracy: 0.9841\n",
      "Epoch 78/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 558ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1079 - val_accuracy: 0.9762\n",
      "Epoch 79/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.9939e-04 - accuracy: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 487ms/step - loss: 3.9939e-04 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 0.9762\n",
      "Epoch 80/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.1112e-04 - accuracy: 1.0000\n",
      "Epoch 00080: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 549ms/step - loss: 6.1112e-04 - accuracy: 1.0000 - val_loss: 0.1238 - val_accuracy: 0.9762\n",
      "Epoch 81/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00081: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 4s 609ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9762\n",
      "Epoch 82/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.0753e-04 - accuracy: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 4s 581ms/step - loss: 4.0753e-04 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9762\n",
      "Epoch 83/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.7322e-04 - accuracy: 1.0000\n",
      "Epoch 00083: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 505ms/step - loss: 5.7322e-04 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.9762\n",
      "Epoch 84/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 491ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9762\n",
      "Epoch 85/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0091e-04 - accuracy: 1.0000\n",
      "Epoch 00085: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 581ms/step - loss: 1.0091e-04 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9841\n",
      "Epoch 86/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.9176e-04 - accuracy: 1.0000\n",
      "Epoch 00086: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 516ms/step - loss: 1.9176e-04 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9941\n",
      "Epoch 00087: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.0067 - accuracy: 0.9941 - val_loss: 0.0340 - val_accuracy: 0.9841\n",
      "Epoch 88/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.6444e-04 - accuracy: 1.0000\n",
      "Epoch 00088: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 5.6444e-04 - accuracy: 1.0000 - val_loss: 0.1248 - val_accuracy: 0.9603\n",
      "Epoch 89/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9941\n",
      "Epoch 00089: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 555ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.1769 - val_accuracy: 0.9603\n",
      "Epoch 90/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9822\n",
      "Epoch 00090: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 556ms/step - loss: 0.0725 - accuracy: 0.9822 - val_loss: 0.1191 - val_accuracy: 0.9603\n",
      "Epoch 91/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9941\n",
      "Epoch 00091: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 559ms/step - loss: 0.0140 - accuracy: 0.9941 - val_loss: 0.0632 - val_accuracy: 0.9921\n",
      "Epoch 92/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9882\n",
      "Epoch 00092: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 0.0314 - accuracy: 0.9882 - val_loss: 0.0523 - val_accuracy: 0.9762\n",
      "Epoch 93/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9941\n",
      "Epoch 00093: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 562ms/step - loss: 0.0238 - accuracy: 0.9941 - val_loss: 0.0314 - val_accuracy: 0.9921\n",
      "Epoch 94/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9763\n",
      "Epoch 00094: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 591ms/step - loss: 0.0566 - accuracy: 0.9763 - val_loss: 0.0312 - val_accuracy: 0.9841\n",
      "Epoch 95/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9941\n",
      "Epoch 00095: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 579ms/step - loss: 0.0730 - accuracy: 0.9941 - val_loss: 0.1869 - val_accuracy: 0.9683\n",
      "Epoch 96/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9941\n",
      "Epoch 00096: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 506ms/step - loss: 0.0213 - accuracy: 0.9941 - val_loss: 0.1926 - val_accuracy: 0.9683\n",
      "Epoch 97/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9941\n",
      "Epoch 00097: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 548ms/step - loss: 0.0196 - accuracy: 0.9941 - val_loss: 0.1069 - val_accuracy: 0.9762\n",
      "Epoch 98/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 0.9941\n",
      "Epoch 00098: val_loss did not improve from 0.01510\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 0.0096 - accuracy: 0.9941 - val_loss: 0.1252 - val_accuracy: 0.9762\n",
      "Epoch 99/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9941\n",
      "Epoch 00099: val_loss improved from 0.01510 to 0.01083, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 708ms/step - loss: 0.0318 - accuracy: 0.9941 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 100/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss improved from 0.01083 to 0.00498, saving model to best_model_improved.h5\n",
      "6/6 [==============================] - 4s 680ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 101/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 579ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9762\n",
      "Epoch 102/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 715ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0728 - val_accuracy: 0.9762\n",
      "Epoch 103/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.4440e-04 - accuracy: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 559ms/step - loss: 9.4440e-04 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9841\n",
      "Epoch 104/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.7318e-04 - accuracy: 1.0000\n",
      "Epoch 00104: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 563ms/step - loss: 1.7318e-04 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 0.9841\n",
      "Epoch 105/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6476e-04 - accuracy: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 675ms/step - loss: 1.6476e-04 - accuracy: 1.0000 - val_loss: 0.0636 - val_accuracy: 0.9841\n",
      "Epoch 106/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.5883e-05 - accuracy: 1.0000\n",
      "Epoch 00106: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 663ms/step - loss: 8.5883e-05 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9841\n",
      "Epoch 107/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.6641e-05 - accuracy: 1.0000\n",
      "Epoch 00107: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 566ms/step - loss: 7.6641e-05 - accuracy: 1.0000 - val_loss: 0.0653 - val_accuracy: 0.9762\n",
      "Epoch 108/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.2146e-05 - accuracy: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 605ms/step - loss: 8.2146e-05 - accuracy: 1.0000 - val_loss: 0.0668 - val_accuracy: 0.9762\n",
      "Epoch 109/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.8757e-05 - accuracy: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 675ms/step - loss: 7.8757e-05 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 0.9841\n",
      "Epoch 110/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.0063e-05 - accuracy: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 761ms/step - loss: 5.0063e-05 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9841\n",
      "Epoch 111/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.9619e-05 - accuracy: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 620ms/step - loss: 5.9619e-05 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9841\n",
      "Epoch 112/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.1365e-05 - accuracy: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 651ms/step - loss: 4.1365e-05 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9841\n",
      "Epoch 113/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.5556e-05 - accuracy: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 737ms/step - loss: 4.5556e-05 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9841\n",
      "Epoch 114/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.8418e-05 - accuracy: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 656ms/step - loss: 3.8418e-05 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9841\n",
      "Epoch 115/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.4912e-05 - accuracy: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 621ms/step - loss: 2.4912e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9841\n",
      "Epoch 116/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - ETA: 0s - loss: 3.0260e-05 - accuracy: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 705ms/step - loss: 3.0260e-05 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9841\n",
      "Epoch 117/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.4203e-05 - accuracy: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 706ms/step - loss: 2.4203e-05 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9841\n",
      "Epoch 118/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.1432e-05 - accuracy: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 602ms/step - loss: 3.1432e-05 - accuracy: 1.0000 - val_loss: 0.0813 - val_accuracy: 0.9841\n",
      "Epoch 119/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6399e-05 - accuracy: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 563ms/step - loss: 1.6399e-05 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9841\n",
      "Epoch 120/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0849e-05 - accuracy: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 618ms/step - loss: 2.0849e-05 - accuracy: 1.0000 - val_loss: 0.0820 - val_accuracy: 0.9841\n",
      "Epoch 121/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8314e-05 - accuracy: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 610ms/step - loss: 1.8314e-05 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9841\n",
      "Epoch 122/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2008e-05 - accuracy: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 621ms/step - loss: 2.2008e-05 - accuracy: 1.0000 - val_loss: 0.0828 - val_accuracy: 0.9841\n",
      "Epoch 123/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.7779e-05 - accuracy: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 552ms/step - loss: 2.7779e-05 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9841\n",
      "Epoch 124/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3343e-05 - accuracy: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 679ms/step - loss: 1.3343e-05 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9841\n",
      "Epoch 125/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.5174e-05 - accuracy: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 576ms/step - loss: 1.5174e-05 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9841\n",
      "Epoch 126/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4649e-05 - accuracy: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 833ms/step - loss: 1.4649e-05 - accuracy: 1.0000 - val_loss: 0.0852 - val_accuracy: 0.9841\n",
      "Epoch 127/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0819e-05 - accuracy: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 633ms/step - loss: 1.0819e-05 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9841\n",
      "Epoch 128/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2060e-05 - accuracy: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 675ms/step - loss: 1.2060e-05 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9841\n",
      "Epoch 129/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0119e-05 - accuracy: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 654ms/step - loss: 2.0119e-05 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9841\n",
      "Epoch 130/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.2623e-06 - accuracy: 1.0000\n",
      "Epoch 00130: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 705ms/step - loss: 9.2623e-06 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9841\n",
      "Epoch 131/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0858e-05 - accuracy: 1.0000\n",
      "Epoch 00131: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 5s 788ms/step - loss: 1.0858e-05 - accuracy: 1.0000 - val_loss: 0.0867 - val_accuracy: 0.9841\n",
      "Epoch 132/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2667e-05 - accuracy: 1.0000\n",
      "Epoch 00132: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 681ms/step - loss: 1.2667e-05 - accuracy: 1.0000 - val_loss: 0.0921 - val_accuracy: 0.9841\n",
      "Epoch 133/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.4900e-06 - accuracy: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 5s 837ms/step - loss: 6.4900e-06 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9841\n",
      "Epoch 134/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.5968e-06 - accuracy: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 5s 800ms/step - loss: 4.5968e-06 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 0.9841\n",
      "Epoch 135/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.0076e-06 - accuracy: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 771ms/step - loss: 7.0076e-06 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9841\n",
      "Epoch 136/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.0610e-06 - accuracy: 1.0000\n",
      "Epoch 00136: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 700ms/step - loss: 5.0610e-06 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9841\n",
      "Epoch 137/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.4764e-06 - accuracy: 1.0000\n",
      "Epoch 00137: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 636ms/step - loss: 7.4764e-06 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 0.9841\n",
      "Epoch 138/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.7622e-06 - accuracy: 1.0000\n",
      "Epoch 00138: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 672ms/step - loss: 6.7622e-06 - accuracy: 1.0000 - val_loss: 0.0936 - val_accuracy: 0.9841\n",
      "Epoch 139/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.2152e-06 - accuracy: 1.0000\n",
      "Epoch 00139: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 755ms/step - loss: 4.2152e-06 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 0.9841\n",
      "Epoch 140/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.0052e-06 - accuracy: 1.0000\n",
      "Epoch 00140: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 636ms/step - loss: 5.0052e-06 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9841\n",
      "Epoch 141/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.0182e-06 - accuracy: 1.0000\n",
      "Epoch 00141: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 5s 813ms/step - loss: 3.0182e-06 - accuracy: 1.0000 - val_loss: 0.0947 - val_accuracy: 0.9841\n",
      "Epoch 142/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.5529e-06 - accuracy: 1.0000\n",
      "Epoch 00142: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 625ms/step - loss: 3.5529e-06 - accuracy: 1.0000 - val_loss: 0.0981 - val_accuracy: 0.9841\n",
      "Epoch 143/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.5978e-06 - accuracy: 1.0000\n",
      "Epoch 00143: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 585ms/step - loss: 2.5978e-06 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 0.9841\n",
      "Epoch 144/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2200e-06 - accuracy: 1.0000\n",
      "Epoch 00144: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 630ms/step - loss: 3.2200e-06 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2283e-06 - accuracy: 1.0000\n",
      "Epoch 00145: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 575ms/step - loss: 2.2283e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9841\n",
      "Epoch 146/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2593e-06 - accuracy: 1.0000\n",
      "Epoch 00146: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 612ms/step - loss: 2.2593e-06 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9841\n",
      "Epoch 147/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6085e-06 - accuracy: 1.0000\n",
      "Epoch 00147: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 601ms/step - loss: 2.6085e-06 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9841\n",
      "Epoch 148/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.9715e-06 - accuracy: 1.0000\n",
      "Epoch 00148: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 565ms/step - loss: 1.9715e-06 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9841\n",
      "Epoch 149/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.3700e-06 - accuracy: 1.0000\n",
      "Epoch 00149: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 571ms/step - loss: 2.3700e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9841\n",
      "Epoch 150/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.6797e-06 - accuracy: 1.0000\n",
      "Epoch 00150: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 608ms/step - loss: 2.6797e-06 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9841\n",
      "Epoch 151/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2290e-06 - accuracy: 1.0000\n",
      "Epoch 00151: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 621ms/step - loss: 2.2290e-06 - accuracy: 1.0000 - val_loss: 0.0983 - val_accuracy: 0.9841\n",
      "Epoch 152/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8234e-06 - accuracy: 1.0000\n",
      "Epoch 00152: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 589ms/step - loss: 1.8234e-06 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 0.9841\n",
      "Epoch 153/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0738e-06 - accuracy: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 556ms/step - loss: 2.0738e-06 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9841\n",
      "Epoch 154/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6717e-06 - accuracy: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 571ms/step - loss: 1.6717e-06 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9841\n",
      "Epoch 155/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.5963e-06 - accuracy: 1.0000\n",
      "Epoch 00155: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 577ms/step - loss: 1.5963e-06 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9841\n",
      "Epoch 156/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0004e-06 - accuracy: 1.0000\n",
      "Epoch 00156: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 515ms/step - loss: 2.0004e-06 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9841\n",
      "Epoch 157/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.7387e-06 - accuracy: 1.0000\n",
      "Epoch 00157: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 597ms/step - loss: 1.7387e-06 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9841\n",
      "Epoch 158/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1632e-06 - accuracy: 1.0000\n",
      "Epoch 00158: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 525ms/step - loss: 1.1632e-06 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9841\n",
      "Epoch 159/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.5779e-06 - accuracy: 1.0000\n",
      "Epoch 00159: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 675ms/step - loss: 1.5779e-06 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9841\n",
      "Epoch 160/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1300e-06 - accuracy: 1.0000\n",
      "Epoch 00160: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 573ms/step - loss: 1.1300e-06 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9841\n",
      "Epoch 161/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2965e-06 - accuracy: 1.0000\n",
      "Epoch 00161: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 569ms/step - loss: 1.2965e-06 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9841\n",
      "Epoch 162/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8290e-06 - accuracy: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 554ms/step - loss: 1.8290e-06 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9841\n",
      "Epoch 163/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3670e-06 - accuracy: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 535ms/step - loss: 1.3670e-06 - accuracy: 1.0000 - val_loss: 0.0966 - val_accuracy: 0.9841\n",
      "Epoch 164/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.3162e-06 - accuracy: 1.0000\n",
      "Epoch 00164: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 625ms/step - loss: 1.3162e-06 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9841\n",
      "Epoch 165/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0150e-06 - accuracy: 1.0000\n",
      "Epoch 00165: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 596ms/step - loss: 1.0150e-06 - accuracy: 1.0000 - val_loss: 0.0966 - val_accuracy: 0.9841\n",
      "Epoch 166/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6026e-06 - accuracy: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 703ms/step - loss: 1.6026e-06 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9841\n",
      "Epoch 167/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.5578e-07 - accuracy: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 541ms/step - loss: 9.5578e-07 - accuracy: 1.0000 - val_loss: 0.1025 - val_accuracy: 0.9841\n",
      "Epoch 168/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4657e-06 - accuracy: 1.0000\n",
      "Epoch 00168: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 644ms/step - loss: 1.4657e-06 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 0.9841\n",
      "Epoch 169/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.8422e-07 - accuracy: 1.0000\n",
      "Epoch 00169: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 593ms/step - loss: 6.8422e-07 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 0.9841\n",
      "Epoch 170/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0672e-06 - accuracy: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 651ms/step - loss: 1.0672e-06 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9841\n",
      "Epoch 171/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4051e-06 - accuracy: 1.0000\n",
      "Epoch 00171: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 682ms/step - loss: 1.4051e-06 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9841\n",
      "Epoch 172/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1505e-06 - accuracy: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 817ms/step - loss: 1.1505e-06 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9841\n",
      "Epoch 173/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1159e-06 - accuracy: 1.0000\n",
      "Epoch 00173: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 660ms/step - loss: 1.1159e-06 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.0799e-06 - accuracy: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 643ms/step - loss: 1.0799e-06 - accuracy: 1.0000 - val_loss: 0.1020 - val_accuracy: 0.9841\n",
      "Epoch 175/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 9.9176e-07 - accuracy: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 597ms/step - loss: 9.9176e-07 - accuracy: 1.0000 - val_loss: 0.1000 - val_accuracy: 0.9841\n",
      "Epoch 176/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.6075e-06 - accuracy: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 649ms/step - loss: 1.6075e-06 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9841\n",
      "Epoch 177/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.0625e-07 - accuracy: 1.0000\n",
      "Epoch 00177: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 811ms/step - loss: 8.0625e-07 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9841\n",
      "Epoch 178/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.4291e-07 - accuracy: 1.0000\n",
      "Epoch 00178: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 697ms/step - loss: 8.4291e-07 - accuracy: 1.0000 - val_loss: 0.1018 - val_accuracy: 0.9841\n",
      "Epoch 179/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 7.7733e-07 - accuracy: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 615ms/step - loss: 7.7733e-07 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9841\n",
      "Epoch 180/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.6972e-07 - accuracy: 1.0000\n",
      "Epoch 00180: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 652ms/step - loss: 8.6972e-07 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9841\n",
      "Epoch 181/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.8193e-07 - accuracy: 1.0000\n",
      "Epoch 00181: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 589ms/step - loss: 5.8193e-07 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9841\n",
      "Epoch 182/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.1140e-07 - accuracy: 1.0000\n",
      "Epoch 00182: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 499ms/step - loss: 5.1140e-07 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9841\n",
      "Epoch 183/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.2746e-07 - accuracy: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 506ms/step - loss: 4.2746e-07 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9841\n",
      "Epoch 184/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.0451e-07 - accuracy: 1.0000\n",
      "Epoch 00184: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 510ms/step - loss: 6.0451e-07 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9841\n",
      "Epoch 185/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.0131e-07 - accuracy: 1.0000\n",
      "Epoch 00185: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 646ms/step - loss: 8.0131e-07 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9841\n",
      "Epoch 186/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 8.0412e-07 - accuracy: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 644ms/step - loss: 8.0412e-07 - accuracy: 1.0000 - val_loss: 0.1041 - val_accuracy: 0.9841\n",
      "Epoch 187/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.7135e-07 - accuracy: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 530ms/step - loss: 5.7135e-07 - accuracy: 1.0000 - val_loss: 0.1043 - val_accuracy: 0.9841\n",
      "Epoch 188/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.2252e-07 - accuracy: 1.0000\n",
      "Epoch 00188: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 486ms/step - loss: 4.2252e-07 - accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9841\n",
      "Epoch 189/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.7825e-07 - accuracy: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 4.7825e-07 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9841\n",
      "Epoch 190/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.0223e-07 - accuracy: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 591ms/step - loss: 5.0223e-07 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9841\n",
      "Epoch 191/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.8969e-07 - accuracy: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 539ms/step - loss: 5.8969e-07 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9841\n",
      "Epoch 192/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.5356e-07 - accuracy: 1.0000\n",
      "Epoch 00192: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 543ms/step - loss: 4.5356e-07 - accuracy: 1.0000 - val_loss: 0.1094 - val_accuracy: 0.9841\n",
      "Epoch 193/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.5247e-07 - accuracy: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 604ms/step - loss: 6.5247e-07 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9841\n",
      "Epoch 194/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2377e-07 - accuracy: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 548ms/step - loss: 3.2377e-07 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9841\n",
      "Epoch 195/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.4824e-07 - accuracy: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 636ms/step - loss: 6.4824e-07 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9841\n",
      "Epoch 196/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.6696e-07 - accuracy: 1.0000\n",
      "Epoch 00196: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 490ms/step - loss: 4.6696e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9841\n",
      "Epoch 197/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 6.2990e-07 - accuracy: 1.0000\n",
      "Epoch 00197: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 512ms/step - loss: 6.2990e-07 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9841\n",
      "Epoch 198/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.5481e-07 - accuracy: 1.0000\n",
      "Epoch 00198: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 498ms/step - loss: 3.5481e-07 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9841\n",
      "Epoch 199/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.4862e-07 - accuracy: 1.0000\n",
      "Epoch 00199: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 531ms/step - loss: 4.4862e-07 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9841\n",
      "Epoch 200/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.8687e-07 - accuracy: 1.0000\n",
      "Epoch 00200: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 563ms/step - loss: 5.8687e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9841\n",
      "Epoch 201/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.4211e-07 - accuracy: 1.0000\n",
      "Epoch 00201: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 545ms/step - loss: 3.4211e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9841\n",
      "Epoch 202/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.7895e-07 - accuracy: 1.0000\n",
      "Epoch 00202: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 646ms/step - loss: 4.7895e-07 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3364e-07 - accuracy: 1.0000\n",
      "Epoch 00203: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 3.3364e-07 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9841\n",
      "Epoch 204/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.8883e-07 - accuracy: 1.0000\n",
      "Epoch 00204: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 524ms/step - loss: 4.8883e-07 - accuracy: 1.0000 - val_loss: 0.1022 - val_accuracy: 0.9841\n",
      "Epoch 205/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6115e-07 - accuracy: 1.0000\n",
      "Epoch 00205: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 479ms/step - loss: 3.6115e-07 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 0.9841\n",
      "Epoch 206/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6045e-07 - accuracy: 1.0000\n",
      "Epoch 00206: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 496ms/step - loss: 3.6045e-07 - accuracy: 1.0000 - val_loss: 0.1014 - val_accuracy: 0.9841\n",
      "Epoch 207/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.8389e-07 - accuracy: 1.0000\n",
      "Epoch 00207: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 572ms/step - loss: 4.8389e-07 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9841\n",
      "Epoch 208/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.9094e-07 - accuracy: 1.0000\n",
      "Epoch 00208: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 529ms/step - loss: 4.9094e-07 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9841\n",
      "Epoch 209/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.0717e-07 - accuracy: 1.0000\n",
      "Epoch 00209: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 577ms/step - loss: 5.0717e-07 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9841\n",
      "Epoch 210/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.8866e-07 - accuracy: 1.0000\n",
      "Epoch 00210: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 570ms/step - loss: 3.8866e-07 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9841\n",
      "Epoch 211/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 5.5301e-07 - accuracy: 1.0000\n",
      "Epoch 00211: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 562ms/step - loss: 5.5301e-07 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9841\n",
      "Epoch 212/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.9501e-07 - accuracy: 1.0000\n",
      "Epoch 00212: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 643ms/step - loss: 3.9501e-07 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9841\n",
      "Epoch 213/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6045e-07 - accuracy: 1.0000\n",
      "Epoch 00213: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 720ms/step - loss: 3.6045e-07 - accuracy: 1.0000 - val_loss: 0.1159 - val_accuracy: 0.9841\n",
      "Epoch 214/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.6115e-07 - accuracy: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 632ms/step - loss: 3.6115e-07 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9841\n",
      "Epoch 215/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.8090e-07 - accuracy: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 547ms/step - loss: 3.8090e-07 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9841\n",
      "Epoch 216/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.2659e-07 - accuracy: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 530ms/step - loss: 3.2659e-07 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9841\n",
      "Epoch 217/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.0771e-07 - accuracy: 1.0000\n",
      "Epoch 00217: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 507ms/step - loss: 4.0771e-07 - accuracy: 1.0000 - val_loss: 0.1153 - val_accuracy: 0.9841\n",
      "Epoch 218/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.7597e-07 - accuracy: 1.0000\n",
      "Epoch 00218: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 554ms/step - loss: 3.7597e-07 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9841\n",
      "Epoch 219/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.8443e-07 - accuracy: 1.0000\n",
      "Epoch 00219: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 551ms/step - loss: 3.8443e-07 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9841\n",
      "Epoch 220/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.5920e-07 - accuracy: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 501ms/step - loss: 4.5920e-07 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9841\n",
      "Epoch 221/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2572e-07 - accuracy: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 501ms/step - loss: 2.2572e-07 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9841\n",
      "Epoch 222/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.1617e-07 - accuracy: 1.0000\n",
      "Epoch 00222: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 508ms/step - loss: 4.1617e-07 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9841\n",
      "Epoch 223/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2078e-07 - accuracy: 1.0000\n",
      "Epoch 00223: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 553ms/step - loss: 2.2078e-07 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9841\n",
      "Epoch 224/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8481e-07 - accuracy: 1.0000\n",
      "Epoch 00224: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 553ms/step - loss: 1.8481e-07 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9841\n",
      "Epoch 225/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.4124e-07 - accuracy: 1.0000\n",
      "Epoch 00225: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 630ms/step - loss: 2.4124e-07 - accuracy: 1.0000 - val_loss: 0.1152 - val_accuracy: 0.9841\n",
      "Epoch 226/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0879e-07 - accuracy: 1.0000\n",
      "Epoch 00226: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 561ms/step - loss: 2.0879e-07 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9841\n",
      "Epoch 227/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4108e-07 - accuracy: 1.0000\n",
      "Epoch 00227: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 556ms/step - loss: 1.4108e-07 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9841\n",
      "Epoch 228/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.1444e-07 - accuracy: 1.0000\n",
      "Epoch 00228: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 558ms/step - loss: 2.1444e-07 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9841\n",
      "Epoch 229/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.9132e-07 - accuracy: 1.0000\n",
      "Epoch 00229: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 616ms/step - loss: 2.9132e-07 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9841\n",
      "Epoch 230/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.9132e-07 - accuracy: 1.0000\n",
      "Epoch 00230: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 567ms/step - loss: 2.9132e-07 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9841\n",
      "Epoch 231/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.3066e-07 - accuracy: 1.0000\n",
      "Epoch 00231: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 553ms/step - loss: 2.3066e-07 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.8003e-07 - accuracy: 1.0000\n",
      "Epoch 00232: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 642ms/step - loss: 2.8003e-07 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9841\n",
      "Epoch 233/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.1850e-07 - accuracy: 1.0000\n",
      "Epoch 00233: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 570ms/step - loss: 1.1850e-07 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9841\n",
      "Epoch 234/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2626e-07 - accuracy: 1.0000\n",
      "Epoch 00234: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 576ms/step - loss: 1.2626e-07 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9841\n",
      "Epoch 235/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 4.3451e-07 - accuracy: 1.0000\n",
      "Epoch 00235: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 600ms/step - loss: 4.3451e-07 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9841\n",
      "Epoch 236/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2838e-07 - accuracy: 1.0000\n",
      "Epoch 00236: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 647ms/step - loss: 1.2838e-07 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9841\n",
      "Epoch 237/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3505e-07 - accuracy: 1.0000\n",
      "Epoch 00237: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 734ms/step - loss: 3.3505e-07 - accuracy: 1.0000 - val_loss: 0.1161 - val_accuracy: 0.9841\n",
      "Epoch 238/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0668e-07 - accuracy: 1.0000\n",
      "Epoch 00238: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 2.0668e-07 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9841\n",
      "Epoch 239/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.0472e-07 - accuracy: 1.0000\n",
      "Epoch 00239: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 646ms/step - loss: 3.0472e-07 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9841\n",
      "Epoch 240/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.8763e-07 - accuracy: 1.0000\n",
      "Epoch 00240: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 627ms/step - loss: 1.8763e-07 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9841\n",
      "Epoch 241/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.2008e-07 - accuracy: 1.0000\n",
      "Epoch 00241: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 701ms/step - loss: 2.2008e-07 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9841\n",
      "Epoch 242/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.3842e-07 - accuracy: 1.0000\n",
      "Epoch 00242: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 605ms/step - loss: 2.3842e-07 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9841\n",
      "Epoch 243/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.5377e-07 - accuracy: 1.0000\n",
      "Epoch 00243: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 508ms/step - loss: 1.5377e-07 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9841\n",
      "Epoch 244/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.0738e-07 - accuracy: 1.0000\n",
      "Epoch 00244: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 575ms/step - loss: 2.0738e-07 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9841\n",
      "Epoch 245/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.7282e-07 - accuracy: 1.0000\n",
      "Epoch 00245: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 1.7282e-07 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9841\n",
      "Epoch 246/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.4531e-07 - accuracy: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 668ms/step - loss: 1.4531e-07 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 0.9841\n",
      "Epoch 247/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.3576e-07 - accuracy: 1.0000\n",
      "Epoch 00247: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 4s 576ms/step - loss: 3.3576e-07 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9841\n",
      "Epoch 248/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 3.9219e-07 - accuracy: 1.0000\n",
      "Epoch 00248: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 519ms/step - loss: 3.9219e-07 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9841\n",
      "Epoch 249/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.8709e-07 - accuracy: 1.0000\n",
      "Epoch 00249: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 538ms/step - loss: 2.8709e-07 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9841\n",
      "Epoch 250/250\n",
      "6/6 [==============================] - ETA: 0s - loss: 2.1020e-07 - accuracy: 1.0000\n",
      "Epoch 00250: val_loss did not improve from 0.00498\n",
      "6/6 [==============================] - 3s 651ms/step - loss: 2.1020e-07 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "history = base_model.fit(train_x_y,\n",
    "                    batch_size=32,\n",
    "                    epochs=250,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_x_y,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45f1f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886443fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_improved.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e816333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cf11b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = image.load_img(r'C:\\Users\\ANANTHU\\Desktop\\solar\\dataset1\\test_set\\Non-Defective\\image_0_492.jpeg')\n",
    "test_image = image.smart_resize(test_image,(64,64),interpolation ='bicubic')\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b51ac8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict= model.predict(test_image)\n",
    "y_class = y_predict.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7cd5b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc3f727b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Defective\n"
     ]
    }
   ],
   "source": [
    "if y_class == 0:\n",
    "    print('Defective')\n",
    "else:\n",
    "    print('Non-Defective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9bb1176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_1 = image.load_img(r'C:\\Users\\ANANTHU\\Desktop\\solar\\dataset1\\test_set\\Defective\\image_0_4175.jpeg')\n",
    "test_image_1 = image.smart_resize(test_image_1,(64,64),interpolation ='bicubic')\n",
    "test_image_1 = image.img_to_array(test_image_1)\n",
    "test_image_1 = np.expand_dims(test_image_1, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0816b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_1= model.predict(test_image_1)\n",
    "y_class_1 = y_predict_1.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93dfffeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_class_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7dc84816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defective\n"
     ]
    }
   ],
   "source": [
    "if y_class_1 == 0:\n",
    "    print('Defective')\n",
    "else:\n",
    "    print('Non-Defective')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d805b589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Achyut\\AppData\\Local\\Temp/ipykernel_860/1359383737.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  base_model.evaluate_generator(generator=train_x_y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.4601273455336923e-06, 1.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate_generator(generator=train_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eaeca3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Achyut\\AppData\\Local\\Temp/ipykernel_860/3492220266.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  base_model.evaluate_generator(generator=test_x_y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1603294461965561, 0.9603174328804016]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate_generator(generator=test_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0982d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd32fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0bedbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc5e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15276c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
